{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Stream Gauges: \n",
    "## Improving Streamflow Predictions in Unmonitored River Segments through Deep Learning\n",
    "\n",
    "This notebook details the development and validation of an advanced Long Short-Term Memory (LSTM) neural network model designed to generate and use synthetic data to improve river streamflow predictions in unmonitored segments. Through the simulation of watershed networks and the application of an innovative LSTM-based approach, this study addresses key challenges in hydrology, such as the lack of monitoring data, and demonstrates how artificial intelligence can significantly contribute to water resource management.\n",
    "\n",
    "To address data sparsity in unmonitored river segments, we generated a synthetic data set representing various hydrologic conditions. Using \"leaky bucket\" models to simulate tributary behavior, this approach allows for a detailed and varied representation of possible flow dynamics. The resulting synthetic data are instrumental in training our LSTM model, providing a solid foundation for learning the complexities of river streamflows and improving prediction capabilities.\n",
    "\n",
    "We developed a balanced in depth and complexity LSTM network to accurately capture the streamflow and overflow dynamics in each of the tributary buckets of the simulated watershed, considering the combined output streamflow as the \"ground truth\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "The first thing to do is setup the notebook environment with all the libraries, declare model global parameters, settings, variables and functions that define the leaky bucket model and the bucket networks we want to represent, as well as define the hyperparameters and structure for the deep learning model. \n",
    "\n",
    "Note: in a typical full-scale modeling framework, these would be decalared in a configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import standard libraries for data management, calculations and plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import machine learning libraries for modeling architectures, training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable \n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import trange, tqdm\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Importing stored data\n",
    "\n",
    "We import the synthetic data generated for the watershed network, which includes the streamflow data for each of the tributary buckets and the combined output streamflow. Or we can generate the synthetic data on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data dictionaries\n",
    "def save_dictionaries(basin_dict, combined_dict, basin_file='data/basin_network_dictionary.pkl', combined_file='data/combined_network_dictionary.pkl'):\n",
    "    with open(basin_file, 'wb') as f:\n",
    "        pickle.dump(basin_dict, f)\n",
    "    with open(combined_file, 'wb') as f:\n",
    "        pickle.dump(combined_dict, f)\n",
    "\n",
    "# Load data dictionaries\n",
    "def load_dictionaries(basin_file='data/basin_network_dictionary.pkl', combined_file='data/combined_network_dictionary.pkl'):\n",
    "    with open(basin_file, 'rb') as f:\n",
    "        basin_dict = pickle.load(f)\n",
    "    with open(combined_file, 'rb') as f:\n",
    "        combined_dict = pickle.load(f)\n",
    "    return basin_dict, combined_dict\n",
    "\n",
    "generate_new_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining the physical bucket model system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**\n",
    "\n",
    "Constants in our bucket model system are:\n",
    "- g, gravitational acceleration [$m/s^2$]\n",
    "- time step [$s$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.80665\n",
    "time_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for the Forcing Process (Precipitation)**\n",
    "\n",
    "To generate precipitation data for our hydrological system, we employ a simple random process that determines whether it is raining based on the previous state, as well as the total amount of rain during the event.\n",
    "\n",
    "We classify precipitation into three types: None, Light, and Heavy. Each type has distinct ranges of rainfall probabilities and depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_probability_range = {\"None\": [0.6, 0.7], \n",
    "                          \"Light\": [0.5, 0.8], \n",
    "                          \"Heavy\": [0.2, 0.3]}\n",
    "\n",
    "rain_depth_range = {\"Light\": [0, 2], \"Heavy\": [2, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucket physical attributes**\n",
    "\n",
    "The physical leaky bucket attributes include:\n",
    "- A_bucket, bucket area [$m^2$]\n",
    "- A_spigot, spigot areas [$m^2$]\n",
    "- H_bucket, bucket heights [$m$]\n",
    "- H_spigot, spigot heights [$m$]\n",
    "- K_infiltration, infiltration rate [$mm/hr$]\n",
    "- ET_parameter, evapotranspiration parameter [$mm/day$]\n",
    "\n",
    "We will generate a diversity of leaky buckets by randomy selecting their physical attribute values from the following possible ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_attributes_range = {\"A_bucket\": [1.0, 2.0],\n",
    "                           \"A_spigot\": [0.1, 0.2],\n",
    "                           \"H_bucket\": [5.0, 6.0],\n",
    "                           \"H_spigot\": [1.0, 3.0],\n",
    "                           \"K_infiltration\": [1e-7, 1e-9],\n",
    "                           \"ET_parameter\": [7, 9]\n",
    "                          }\n",
    "\n",
    "bucket_attributes_list = list(bucket_attributes_range.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Channel physical parameters**\n",
    "\n",
    "This section includes the channel physical parameters. Initially, the transformation parameters are defined\n",
    "\n",
    "- transform_type: 0 (null transform), 1 (simple shift), 2 (time-based displacement), 3 (simple attenuation), 4 (LagK), 5 (Muskingum-Cunge)\n",
    "- shift_amount: position number to shift (applies when transform_type == 1)\n",
    "- time_shift: specified number of seconds to move the streamflow\n",
    "- attenuation_factor: value for multiplying the streamflow\n",
    "- lag_amount: number of streamflow positions to delay the output\n",
    "- param_list: list of parameters to apply Muskingum-Cunge transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_params_range = {\"transform_type\": [0], #\"transform_type\": [0, 1, 3, 4, 5], #transform_types 1 to 5 are not completely implemented\n",
    "                          \"shift_amount\": [0, 100],\n",
    "                          # \"time_shift\": [],\n",
    "                          \"attenuation_factor\": [0.01, 0.99],\n",
    "                          \"lag_amount\": [0, 100]\n",
    "                         }\n",
    "\n",
    "transform_params_list = list(transform_params_range.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model input and output variables**\n",
    "\n",
    "The data fluxes in and out of the leaky bucket considered in this stystem and the leaky bucket state, include\n",
    "- precipitation into the bucket as a model input (precip) \n",
    "- the actual and potential loss to evaporation from the bucket as model inputs (et, pet)\n",
    "- the water flow over the bucket and out of the spigot as a simulated model output(q_overflow, q_spigot)\n",
    "- and the state of the water head in the bucket as a simluated model output (h_bucket).\n",
    "\n",
    "We define lists based on these input and output leaking bucket model variables.\n",
    "\n",
    "n_combined_input is the number of features to use in the LSTM model. \n",
    "n_combined_output is the number of features to predict per each bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_buckets_per_network = 2\n",
    "\n",
    "input_vars = ['precip', 'et', 'q_total_output']\n",
    "input_vars.extend(bucket_attributes_list)\n",
    "n_input = len(input_vars) \n",
    "\n",
    "# For combined bucket network inputs with combined downstream flow data\n",
    "input_combined_vars = [f\"{var}_b{bucket}\" for bucket in range(n_buckets_per_network) for var in ['precip', 'et']]\n",
    "input_combined_vars.append('q_total_output')\n",
    "for attribute in bucket_attributes_list:\n",
    "    for bucket in range(n_buckets_per_network):\n",
    "        input_combined_vars.append(f\"{attribute}_b{bucket}\")\n",
    "output_combined_vars = [f\"{var}_b{bucket}\" for bucket in range(n_buckets_per_network) for var in ['q_overflow', 'q_spigot']]\n",
    "n_combined_input = len(input_combined_vars)\n",
    "n_combined_output = len(output_combined_vars)\n",
    "\n",
    "print(\"input_combined_vars: \" + str(input_combined_vars))\n",
    "print(\"n_combined_input: \" + str(n_combined_input))\n",
    "print(\"output_combined_vars: \" + str(output_combined_vars))\n",
    "print(\"n_combined_output: \" + str(n_combined_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data noise**\n",
    "\n",
    "Because real world systems are noisy, we can add noise to the synthetic data by multiplying the values by a random factor taken from a normal distribution with a mean of 1 and a standard deviation prescribed for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noise = True\n",
    "noise = {\"pet\": 0.1, \"et\": 0.1, \"q\": 0.1, \"head\": 0.1} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration of leaking bucket hydrological system**\n",
    "\n",
    "![Illustration of leaking bucket hydrological system](figs/Leaky_Bucket_Model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Defining the modeling setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning Model (LSTM) Hyperparameters**\n",
    "\n",
    "The hyperparameters for the LSTM (Long Short-Term Memory) deep learning model include:\n",
    "- device: The device (CPU or CUDA or MPS) used for training and inference.\n",
    "- hidden_state_size: The number of units in the hidden state of the LSTM model.\n",
    "- num_layers: Number of LSTM layers.\n",
    "- num_epochs: Number of training epochs.\n",
    "- batch_size: Number of samples in each training batch.\n",
    "- seq_length: Length of input sequences.\n",
    "- learning_rate: Learning rate for the optimizer.\n",
    "- num_classes: Number of output classes.\n",
    "- input_size: Size of the input layer.\n",
    "\n",
    "These hyperparameters control the behavior and performance of the LSTM model and can be adjusted to optimize the model's accuracy and generalization capabilities for specific tasks and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic selection of device: MPS for Apple Silicon, CUDA for NVIDIA GPUs if available, otherwise CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# Specific configuration for MPS\n",
    "if device.type == 'mps':\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    print(\"MPS fallback enabled:\", os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK'))\n",
    "\n",
    "# Model parameters\n",
    "batch_size = 256 \n",
    "seq_length = 24\n",
    "num_epochs_combined = 30\n",
    "num_layers_combined = 60\n",
    "hidden_state_size_combined = 36\n",
    "learning_rate_combined = np.linspace(start=0.1, stop=0.001, num=num_epochs_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Size of data records and splits**\n",
    "\n",
    "The deep learning framework requires the data to be split into 3 different sets\n",
    "- Back propogation is done on the training set (train)\n",
    "- Hyperparameter tuning is done on the validation set (val)\n",
    "- Calculating model accuracy is done on the testing set (test)\n",
    "\n",
    "We will define how many randomly generated basin network configurations will be used for the \"train\", \"val\" and \"test\" sets, as well as the length of the simulations for each set.\n",
    "\n",
    "Additionally it is necessary to define the number of buckets in every basin network (n_bucket_per_network). Consequently, the number of buckets is equal to the number of basin networks multiplied by the number of buckets per network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_splits = {\"train\": 7000, \"val\": 1500,\"test\": 1500}\n",
    "n_basin_networks_split = {\"train\": 350, \"val\": 204, \"test\": 204}\n",
    "n_buckets_split = {\"train\": n_basin_networks_split[\"train\"] * n_buckets_per_network, \"val\": n_basin_networks_split[\"val\"] * n_buckets_per_network,\"test\": n_basin_networks_split[\"test\"] * n_buckets_per_network}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the settings above to calculate the total length of the data record to generate and the total number of buckets and basin networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = time_splits[\"train\"] + time_splits[\"val\"] + time_splits[\"test\"] + seq_length * 3\n",
    "n_buckets = n_buckets_split[\"train\"] + n_buckets_split[\"val\"] + n_buckets_split[\"test\"]\n",
    "n_basin_networks = n_basin_networks_split[\"train\"] + n_basin_networks_split[\"val\"] + n_basin_networks_split[\"test\"]\n",
    "print(n_basin_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the parameters for the bucket and time splits necessary to feed the model with separate datasets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a new function to calculate parameter for the basin network splits into separate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_parameters():\n",
    "    # create lists of bucket indices for each set based on the given bucket splits\n",
    "    buckets_for_training = list(range(0, n_buckets_split['train'] + 1))\n",
    "    buckets_for_val = list(range(n_buckets_split['train'] + 1, \n",
    "                                 n_buckets_split['train'] + n_buckets_split['val'] + 1))\n",
    "    buckets_for_test = list(range(n_buckets - n_buckets_split['test'], n_buckets))\n",
    "\n",
    "    # determine the time range for each set based on the given time splits\n",
    "    train_start = seq_length\n",
    "    train_end   = time_splits[\"train\"]\n",
    "    val_start   = train_end + seq_length\n",
    "    val_end     = val_start + time_splits[\"val\"]\n",
    "    test_start  = val_end + seq_length\n",
    "    test_end    = test_start + time_splits[\"test\"]\n",
    "    \n",
    "    # organize the split parameters into separate lists for each set\n",
    "    train_split_parameters = [buckets_for_training, train_start, train_end]\n",
    "    val_split_parameters = [buckets_for_val, val_start, val_end]\n",
    "    test_split_parameters = [buckets_for_test, test_start, test_end]\n",
    "    \n",
    "    return [train_split_parameters, val_split_parameters, test_split_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[buckets_for_training, train_start, train_end],\n",
    "[buckets_for_val, val_start, val_end],\n",
    "[buckets_for_test, test_start, test_end]]= split_parameters()\n",
    "print([[buckets_for_training, train_start, train_end],\n",
    "[buckets_for_val, val_start, val_end],\n",
    "[buckets_for_test, test_start, test_end]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create separate datasets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_split_param():\n",
    "    #create lists of basin indices for each set\n",
    "    basins_train = list(range(0, n_basin_networks_split['train']))\n",
    "    basins_val = list(range(n_basin_networks_split['train'], \n",
    "                            n_basin_networks_split['train'] + n_basin_networks_split['val']))\n",
    "    basins_test = list(range(n_basin_networks - n_basin_networks_split['test'], n_basin_networks)) \n",
    "    \n",
    "    #place basin split parameters into separate lists\n",
    "    train_basin_split_param = [basins_train]\n",
    "    val_basin_split_param = [basins_val]\n",
    "    test_basin_split_param = [basins_test]\n",
    "    \n",
    "    #determine transform parameters per basin network\n",
    "    \n",
    "    \n",
    "    return [train_basin_split_param, val_basin_split_param, test_basin_split_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run the function above and print out the range for the whole basin network split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[basin_networks_train],[basin_networks_val],[basin_networks_test]]= basin_split_param()\n",
    "print([[basin_networks_train],[basin_networks_val],[basin_networks_test]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Creating a sample of diverse buckets\n",
    "\n",
    "Now that we know how many buckets we need, we can generate a sample of diverse buckets with their respective boundary and initial conditions by randomly sampling from the possible ranges for each attribute defined in the settings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_buckets(n_buckets):\n",
    "    # Boundary conditions\n",
    "    buckets = {bucket_attribute:[] for bucket_attribute in bucket_attributes_list}\n",
    "    for i in range(n_buckets):\n",
    "        for attribute in bucket_attributes_list:\n",
    "            buckets[attribute].append(np.random.uniform(bucket_attributes_range[attribute][0], \n",
    "                                                        bucket_attributes_range[attribute][1]))\n",
    "\n",
    "    # Initial conditions\n",
    "    h_water_level = [np.random.random() for i in range(n_buckets)]\n",
    "    mass_overflow = [np.random.random() for i in range(n_buckets)]\n",
    "    return buckets, h_water_level, mass_overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets, h_water_level, mass_overflow = setup_buckets(n_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Creating the synthetic \"precipitation\" \n",
    "\n",
    "We first randomly assign rainfall parameters for each bucket, using the probability ranges specified for each precipitation type. We then generate synthetic input time series for each bucket model, known as forcing data. This process involves using the previously defined rainfall parameters and the random process defined in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_rain_params():\n",
    "    buck_rain_params = [rain_depth_range,\n",
    "                        np.random.uniform(rain_probability_range[\"None\"][0],\n",
    "                                            rain_probability_range[\"None\"][1]),\n",
    "                        np.random.uniform(rain_probability_range[\"Heavy\"][0],\n",
    "                                            rain_probability_range[\"Heavy\"][1]),\n",
    "                        np.random.uniform(rain_probability_range[\"Light\"][0],\n",
    "                                            rain_probability_range[\"Light\"][1])\n",
    "                 ]\n",
    "    return buck_rain_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rain(preceding_rain, bucket_rain_params):\n",
    "    depth_range, no_rain_probability, light_rain_probability, heavy_rain_probability = bucket_rain_params\n",
    "    # some percent of time we have no rain at all\n",
    "    if np.random.uniform(0.01, 0.99) < no_rain_probability:\n",
    "        rain = 0\n",
    "\n",
    "    # When we do have rain, the probability of heavy or light rain depends on the previous day's rainfall\n",
    "    else:\n",
    "        # If yesterday was a light rainy day, or no rain, then we are likely to have light rain today\n",
    "        if preceding_rain < depth_range[\"Light\"][1]:\n",
    "            if np.random.uniform(0, 1) < light_rain_probability:\n",
    "                rain = np.random.uniform(0, 1)\n",
    "            else:\n",
    "                # But if we do have heavy rain, then it could be very heavy\n",
    "                rain = np.random.uniform(depth_range[\"Heavy\"][0], depth_range[\"Heavy\"][1])\n",
    "\n",
    "        # If it was heavy rain yesterday, then we might have heavy rain again today\n",
    "        else:\n",
    "            if np.random.uniform(0, 1) < heavy_rain_probability:\n",
    "                rain = np.random.uniform(0, 1)\n",
    "            else:\n",
    "                rain = np.random.uniform(depth_range[\"Light\"][0], depth_range[\"Light\"][1])\n",
    "    return rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a random rainfall input timeseries for each bucket and store it in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_list = {}\n",
    "for ibuc in range(n_buckets):\n",
    "    bucket_rain_params = pick_rain_params()\n",
    "    in_list[ibuc] = [0]\n",
    "    for i in range(1, num_records):\n",
    "        in_list[ibuc].append(random_rain(in_list[ibuc][i-1], bucket_rain_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Running Numerical Simulations of the Bucket Model to Generate \"Ground Truth\" Data\n",
    "\n",
    "We run bucket model simulations to generate the data for our system. \n",
    "\n",
    "We perform numerical simulations of a bucket model for a specific bucket index. It iterates over each time step, updating the water level based on precipitation, evapotranspiration, infiltration, overflow, and spigot outflow. The simulation results, including the water level, overflow, spigot flow, and other attributes, are stored in a data frame. This is a concise representation of the bucket model's behavior over time for the given bucket index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bucket_simulation(ibuc, noise):\n",
    "    columns = ['precip', 'et', 'h_bucket', 'q_overflow', 'q_spigot', 'q_total']\n",
    "    columns.extend(bucket_attributes_list)\n",
    "    \n",
    "    # Precompute g (acceleration due to gravity)\n",
    "    # g = 9.81\n",
    "\n",
    "    # Pre-generate random numbers for noise\n",
    "    if is_noise:\n",
    "        pet_noise = np.random.normal(1, noise['pet'], len(in_list[ibuc]))\n",
    "        et_noise = np.random.normal(1, noise['et'], len(in_list[ibuc]))\n",
    "        head_noise = np.random.normal(1, noise['head'], len(in_list[ibuc]))\n",
    "        q_noise = np.random.uniform(0, noise['q'], len(in_list[ibuc]))\n",
    "\n",
    "    # Memory to store model results\n",
    "    data = []\n",
    "\n",
    "    # Main loop through time\n",
    "    for t, precip_in in enumerate(in_list[ibuc]):\n",
    "        # Add the input mass to the bucket\n",
    "        h_water_level[ibuc] = h_water_level[ibuc] + precip_in\n",
    "\n",
    "        # Lose mass out of the bucket (evaporation and infiltration)\n",
    "        et = np.max([0, (buckets[\"A_bucket\"][ibuc] / buckets[\"ET_parameter\"][ibuc]) * np.sin(t) * pet_noise[t]])\n",
    "        infiltration = h_water_level[ibuc] * buckets[\"K_infiltration\"][ibuc]\n",
    "        h_water_level[ibuc] = np.max([0, h_water_level[ibuc] - et])\n",
    "        h_water_level[ibuc] = np.max([0, h_water_level[ibuc] - infiltration])\n",
    "        if is_noise:\n",
    "            h_water_level[ibuc] *= et_noise[t]\n",
    "\n",
    "        # Overflow if the bucket is too full\n",
    "        if h_water_level[ibuc] > buckets[\"H_bucket\"][ibuc]:\n",
    "            mass_overflow[ibuc] = h_water_level[ibuc] - buckets[\"H_bucket\"][ibuc]\n",
    "            h_water_level[ibuc] = buckets[\"H_bucket\"][ibuc]\n",
    "            if is_noise:\n",
    "                h_water_level[ibuc] -= q_noise[t]\n",
    "\n",
    "        # Calculate head on the spigot\n",
    "        h_head_over_spigot = h_water_level[ibuc] - buckets[\"H_spigot\"][ibuc]\n",
    "        if is_noise:\n",
    "            h_head_over_spigot *= head_noise[t]\n",
    "\n",
    "        # Calculate water leaving the bucket through the spigot\n",
    "        if h_head_over_spigot > 0:\n",
    "            velocity_out = np.sqrt(2 * g * h_head_over_spigot)\n",
    "            spigot_out = velocity_out * buckets[\"A_spigot\"][ibuc] * time_step\n",
    "            h_water_level[ibuc] -= spigot_out\n",
    "        else:\n",
    "            spigot_out = 0\n",
    "\n",
    "        # Save the data in time series\n",
    "        data.append([precip_in, et, h_water_level[ibuc], mass_overflow[ibuc], spigot_out, mass_overflow[ibuc] + spigot_out] +\n",
    "                    [buckets[attribute][ibuc] for attribute in bucket_attributes_list])\n",
    "\n",
    "        mass_overflow[ibuc] = 0\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run and store the simulations for each bucket in a dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran basin network model simulation to create a dictionary that contains n_basin_networks elements. Each element represents a network of n_buckets_per_network buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basin_network_simultation():\n",
    "    result = {}\n",
    "    result['buckets'] = {}\n",
    "    for i in range(n_basin_networks):\n",
    "        bucket_dictionary = {}\n",
    "        for j in range(n_buckets_per_network):\n",
    "            bucket_dictionary[j] = run_bucket_simulation((i * n_buckets_per_network) + j, noise)\n",
    "        result['buckets'][i] = bucket_dictionary\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_network_dictionary = {}\n",
    "\n",
    "if generate_new_data or not (os.path.exists('data/basin_network_dictionary.pkl') and os.path.exists('data/combined_network_dictionary.pkl')):\n",
    "    basin_network_dictionary = run_basin_network_simultation()\n",
    "else:\n",
    "    basin_network_dictionary, _ = load_dictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.7 Create a combined streamflow\n",
    "\n",
    "We create a function to transform streamflow. \n",
    "\n",
    "The data in each bucket network and these combined streamflow represent the \"ground truth\", which is what we will try to learn with the LSTM. \n",
    "\n",
    "The transformation type to perform can be one of the following:\n",
    "\n",
    "- **Null transformation:** Returns the same series of input streamflows\n",
    "- **Simple shift:** Move the elements right or left according to parameters shift_amount\n",
    "- **Time-based shift:** Move the elements to the right or left according to the specified number of seconds time_shift; requires some information about the internal time values of the series.\n",
    "- **Simple Attenuation:** multiply each element of the series by an attenuation factor\n",
    "- **LagK:** Move each element to position K (lag_amount)\n",
    "- **Muskingum-Cunge:** Muskingum-Cunge method to combine river flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mcTransform(Q, param_list):\n",
    "#    return Q\n",
    "\n",
    "def transformQ(Q, T):\n",
    "    # Verify if the transformation parameters are null\n",
    "    if T[0] == 0 or T[0] is None:\n",
    "        # The null transformation returns the same series of input streamflows\n",
    "        return Q\n",
    "    else:\n",
    "        transform_type = T[0]\n",
    "        if transform_type == 1:\n",
    "            # Implement the simple shift transformation\n",
    "            # Move the elements to the right or left according to parameters\n",
    "            shift_amount = T[1]\n",
    "            #print(shift_amount)\n",
    "            #print(Q[shift_amount:])\n",
    "            #print(Q[:shift_amount])\n",
    "            #transformed_Q = Q[shift_amount:] + Q[:shift_amount]\n",
    "            transformed_Q = np.roll(Q, shift_amount).tolist()\n",
    "        elif transform_type == 2:\n",
    "            # Implementing time-based displacement transformation\n",
    "            # Move the elements to the right or left according to the specified number of seconds\n",
    "            time_shift = T[1]\n",
    "            # Information about the internal time values of the series should be used here\n",
    "            # to calculate the appropriate displacement based on the specified seconds\n",
    "            # transformed_Q = ...\n",
    "            # Implement the code for this transformation according to needs and available data\n",
    "        elif transform_type == 3:\n",
    "            # Implement the simple attenuation transformation\n",
    "            # Multiplying each element of the series by an attenuation factor\n",
    "            attenuation_factor = T[1]\n",
    "            transformed_Q = [q * attenuation_factor for q in Q]\n",
    "        elif transform_type == 4:\n",
    "            # Implement LagK transformation\n",
    "            # Move each element to position K\n",
    "            lag_amount = T[1]\n",
    "            transformed_Q = [Q[i - lag_amount] if i >= lag_amount else 0 for i in range(len(Q))]\n",
    "        elif transform_type == 5:\n",
    "            # Implement the Muskingum-Cunge method to combine river flows\n",
    "            param_list = T[1]\n",
    "            # transformed_Q = mcTransform(Q, param_list)\n",
    "            # Implement the code for this transformation according to needs and available data\n",
    "        #else:\n",
    "            # In case new transformations are added in the future, they can be handled here\n",
    "            # by adding more elif blocks\n",
    "        return transformed_Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the transformed streamflows of a basin network in one single streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_key(transform_type):\n",
    "    if transform_type == 0:\n",
    "        return None\n",
    "    elif transform_type == 1:\n",
    "        return \"shift_amount\"\n",
    "    #elif transform_type == 2:  # For time-based transformation (not implemented yet)\n",
    "    #    return \"time_shift\"\n",
    "    elif transform_type == 3:\n",
    "        return \"attenuation_factor\"\n",
    "    elif transform_type == 4:\n",
    "        return \"lag_amount\"\n",
    "    #elif tranform_type == 5:  # For Muskingum-Cunge transformation (not implemented yet)\n",
    "    #    return \"param_list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transform_params(bucket_basin):\n",
    "    n = len(bucket_basin)\n",
    "    transform_params = []\n",
    "    for i in range(n - 1):\n",
    "        transform_type = np.random.choice(transform_params_range[\"transform_type\"])\n",
    "        param_key = get_param_key(transform_type)\n",
    "        if transform_type == 0:\n",
    "            param_value = None\n",
    "        else:\n",
    "            param_range = transform_params_range[param_key]\n",
    "            #param_value = None\n",
    "            if param_key in [\"shift_amount\", \"lag_amount\"]:\n",
    "                param_value = np.random.randint(param_range[0], param_range[1] + 1)\n",
    "            else:\n",
    "                param_value = np.random.uniform(param_range[0], param_range[1])\n",
    "        transform_params.append((transform_type, param_value))\n",
    "    transform_params.append((0, None))\n",
    "    return transform_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transform_params_network(basin_network_dictionary):\n",
    "    transform_params_vector = []\n",
    "    for i in range(n_basin_networks):\n",
    "        transform_params_vector.append([i, generate_transform_params(basin_network_dictionary['buckets'][i])])\n",
    "    return transform_params_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_params_vector = generate_transform_params_network(basin_network_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_streamflows(flows, transform_params):\n",
    "    transformed_flows = []\n",
    "    \n",
    "    for Q, T in zip(flows, transform_params):\n",
    "        transformed_flows.append(transformQ(Q, T))\n",
    "\n",
    "    return np.sum(transformed_flows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def streamflows_output(transform_params_vector):\n",
    "    combined_streamflows_dictionary = {} \n",
    "    q_total_matrix = []\n",
    "    for i in range(n_basin_networks):\n",
    "        q_total_vector = []\n",
    "        for j in range(n_buckets_per_network):\n",
    "            q_total_vector.append(basin_network_dictionary['buckets'][i][j]['q_total'])\n",
    "        q_total_matrix.append(combine_streamflows(q_total_vector, transform_params_vector[i][1]))\n",
    "    combined_streamflows_dictionary['q_total_output'] = q_total_matrix\n",
    "    return combined_streamflows_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_streamflows_dictionary = streamflows_output(transform_params_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_streamflows_dictionary['q_total_output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inetwork in range(n_basin_networks):\n",
    "    q_total_output_values = combined_streamflows_dictionary['q_total_output'][inetwork]\n",
    "    \n",
    "    for ibuc in range(n_buckets_per_network):\n",
    "        bucket_df = basin_network_dictionary['buckets'][inetwork][ibuc]\n",
    "        bucket_df['q_total_output'] = q_total_output_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basin_network_dictionary['buckets'][0][0].keys())\n",
    "print(basin_network_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined network dictionary\n",
    "combined_network_dictionary = {'networks': {}}\n",
    "\n",
    "if generate_new_data or not (os.path.exists('data/basin_network_dictionary.pkl') and os.path.exists('data/combined_network_dictionary.pkl')):\n",
    "\n",
    "    for network_id in basin_network_dictionary['buckets'].keys():\n",
    "        combined_df_list = [] \n",
    "        q_total_output_series = None  \n",
    "\n",
    "        for bucket_id in range(n_buckets_per_network):\n",
    "            df = basin_network_dictionary['buckets'][network_id][bucket_id]\n",
    "            if q_total_output_series is None:\n",
    "                q_total_output_series = df['q_total_output']\n",
    "            df = df.rename(columns={col: f\"{col}_b{bucket_id}\" for col in df.columns if col != 'q_total_output'})\n",
    "            df = df.drop(columns=['q_total_output'])\n",
    "            combined_df_list.append(df)\n",
    "\n",
    "        combined_df = pd.concat(combined_df_list, axis=1)\n",
    "        combined_df['q_total_output'] = q_total_output_series\n",
    "        combined_network_dictionary['networks'][network_id] = combined_df\n",
    "else:\n",
    "    _, combined_network_dictionary = load_dictionaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_new_data or not (os.path.exists('data/basin_network_dictionary.pkl') and os.path.exists('data/combined_network_dictionary.pkl')):\n",
    "    save_dictionaries(basin_network_dictionary, combined_network_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_network_dictionary['networks'][0].keys())\n",
    "print(combined_network_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function retrieves data from a specific bucket within a given basin network. It takes three arguments:\n",
    "- **inetwork**: The index of the watershed network from which data will be retrieved.\n",
    "- **ibuc**: The index of the specific bucket within the basin network.\n",
    "- **basin_network_dictionary**: A dictionary containing all the basin networks and their respective buckets.\n",
    "\n",
    "The function returns a DataFrame **df** containing the data of the specified bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(inetwork, ibuc, basin_network_dictionary):\n",
    "    df = basin_network_dictionary['buckets'][inetwork][ibuc]\n",
    "    return df\n",
    "\n",
    "def retrieve_combined_data(inetwork, combined_network_dictionary):\n",
    "    df = combined_network_dictionary['networks'][inetwork]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.8 Visualizing a sample of the bucket fluxes\n",
    "We plot the model simulations to ensure the existence of fluxes and validate that the generated values are realistics for both the spigot (channel flow) and over the top (flooding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_simulation(basin_network_dictionary, inetwork, ibuc):\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    print('Bucket:', ibuc)\n",
    "    print(\"Overflow mean:\", np.round(basin_network_dictionary['buckets'][inetwork][ibuc].q_overflow.mean(),2))\n",
    "    print(\"Overflow max:\", np.round(basin_network_dictionary['buckets'][inetwork][ibuc].q_overflow.max(),2))\n",
    "\n",
    "    input_vars_for_plot = [var for var in input_vars if var != 'q_total_output']\n",
    "    \n",
    "    basin_network_dictionary['buckets'][inetwork][ibuc].loc[:100, input_vars_for_plot].plot(ax=ax1)\n",
    "    basin_network_dictionary['buckets'][inetwork][ibuc].loc[:100, ['q_overflow', 'q_spigot','h_bucket']].plot(ax=ax2)\n",
    "\n",
    "    ax1.set_title('Model inputs')\n",
    "    ax1.set_xlabel('Time (hours)')\n",
    "    ax1.set_ylabel('Values')\n",
    "    ax2.set_title('Model outputs')\n",
    "    ax2.set_xlabel('Time (hours)')\n",
    "    ax2.set_ylabel('Flow/Level')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ibuc in range(n_buckets_per_network):\n",
    "    viz_simulation(basin_network_dictionary, n_basin_networks_split['train'], ibuc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the combined flow of a network to verify the consistency of the transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_network(basin_network_dictionary, inetwork):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for ibuc in range(n_buckets_per_network):\n",
    "        print('Bucket:', ibuc)\n",
    "        print(\"Total mean:\", np.round(basin_network_dictionary['buckets'][inetwork][ibuc].q_total.mean(),2))\n",
    "        print(\"Total max:\", np.round(basin_network_dictionary['buckets'][inetwork][ibuc].q_total.max(),2))\n",
    "\n",
    "        plt.plot(basin_network_dictionary['buckets'][inetwork][ibuc].loc[:100,\"q_total\"], label='Bucket '+str(ibuc)+' q_total')\n",
    "        \n",
    "    q_total_output = basin_network_dictionary['buckets'][inetwork][0].loc[:100,\"q_total_output\"]\n",
    "\n",
    "    plt.plot(q_total_output, color='k', linestyle='dashed', label='q_total_output')\n",
    "\n",
    "    plt.title('Combined streamflows per Network ' + str(inetwork) + ': q_total = q_spigot + q_overflow')\n",
    "    plt.xlabel('Time (hours)')\n",
    "    plt.ylabel('Flow rate (m$^3$/s)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countN = 0\n",
    "for inetwork in basin_networks_val:\n",
    "    countN += 1\n",
    "    viz_network(basin_network_dictionary, inetwork)\n",
    "    if countN == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep learning model\n",
    "This section sets up our deep learning model and training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining the neural network model\n",
    "This is the part of the notebook where we will be using the previous simulations to learn from the generated data and subsequently make predictions. Here, we leverage the simulations to extract valuable insights and apply them in our learning and prediction processes.\n",
    "\n",
    "Here we define a class called LSTM1, which is a PyTorch module for a multi-layer Long Short-Term Memory (LSTM) network. \n",
    "\n",
    "**Architecture**\n",
    "\n",
    "![Illustration of LSTM Network Architecture](figs/LSTM_Architecture.png)\n",
    "\n",
    "**Brief explanation**\n",
    "- The input to the module is a tensor x of shape (`batch_size`, `seq_length`, `input_size`), which represents a sequence of batch_size samples, each of length seq_length, with input_size features at each time step. \n",
    "- The LSTM layer is defined using the nn.LSTM class, with input_size as the size of the input layer, hidden_size as the size of the hidden state, and batch_first=True indicating that the first dimension of the input tensor is the batch size. \n",
    "- The output of the LSTM layer is passed through a ReLU activation function, and then to a fully connected layer (nn.Linear) with num_classes output units. \n",
    "- The forward method takes the input tensor x as an argument, along with an optional tuple `init_states` representing the initial hidden and internal states of the LSTM layer, and returns the output tensor prediction. \n",
    "- If `init_states` is not provided, it is initialized as a tensor of zeros with shape (`batch_size`, `hidden_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, batch_size, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_1 =  nn.Linear(hidden_size, num_classes) #fully connected 1\n",
    "   \n",
    "    def forward(self, x, init_states=None):\n",
    "\n",
    "        if init_states is None:\n",
    "            h_t = Variable(torch.zeros(1, x.size(0), self.hidden_size, device=device)) # hidden state\n",
    "            c_t = Variable(torch.zeros(1, x.size(0), self.hidden_size, device=device)) # internal state\n",
    "            init_states = (h_t, c_t)\n",
    "           \n",
    "        out, _ = self.lstm(x, init_states)\n",
    "        out = self.relu(out)\n",
    "        prediction = self.fc_1(out) # Dense, fully connected layer\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining a procedure for model validation\n",
    "Here we verify that our model is working the way we expect. We would especially want to check model validation when changing hyperparameters.\n",
    "\n",
    "We define a function that validates and tests the LSTM model, as well as checks the water balance of the system. \n",
    "\n",
    "**Brief explanation**\n",
    "- We use the pre-defined LSTM model to make predictions on the validation data. \n",
    "- The output of this model is then used to compute four different metrics, the Nash-Sutcliffe Efficiency (NSE), Root Mean Sqaure Error (RMSE), Mean Absolute Error (MAE) for the spigot_out, and  mass_overflow columns of the dataframe.\n",
    "- We plot the actual spigot_out and mass_overflow values against their corresponding LSTM predictions. \n",
    "- We check the water balance of the system by summing up the input, evapotranspiration, mass_overflow, spigot_out, and the last recorded water level in the dataframe, and compare this to the total mass out of or left in the system. \n",
    "- We print out the percent mass residual as a measure of how well the system is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validation_period_combined(lstm_combined, combined_np_val_seq_X, combined_network_dictionary, inetwork, n_plot=100, verbose=False):\n",
    "    def __make_prediction(df, ibuc):\n",
    "        lstm_output_val = lstm_combined(torch.Tensor(combined_np_val_seq_X[inetwork]).to(device=device))\n",
    "        val_spigot_prediction = []\n",
    "        val_overflow_prediction = []\n",
    "\n",
    "        for i in range(lstm_output_val.shape[0]):\n",
    "            spigot_pred = (lstm_output_val[i, -1, (ibuc * 2) + 1].cpu().detach().numpy() * np.std(df.loc[train_start:train_end, 'q_spigot_b' + str(ibuc)])) + np.mean(df.loc[train_start:train_end, 'q_spigot_b' + str(ibuc)])\n",
    "            overflow_pred = (lstm_output_val[i, -1, (ibuc * 2)].cpu().detach().numpy() * np.std(df.loc[train_start:train_end, 'q_overflow_b' + str(ibuc)])) + np.mean(df.loc[train_start:train_end, 'q_overflow_b' + str(ibuc)])\n",
    "\n",
    "            #spigot_pred = (lstm_output_val[i, -1, (ibuc * 2) + 1].cpu().detach().numpy() * spigot_std) + spigot_mean\n",
    "            #overflow_pred = (lstm_output_val[i, -1, (ibuc * 2)].cpu().detach().numpy() * overflow_std) + overflow_mean\n",
    "\n",
    "            spigot_pred = max(spigot_pred, 0)\n",
    "            overflow_pred = max(overflow_pred, 0)\n",
    "\n",
    "            val_spigot_prediction.append(spigot_pred)\n",
    "            val_overflow_prediction.append(overflow_pred)\n",
    "\n",
    "        return val_spigot_prediction, val_overflow_prediction\n",
    "\n",
    "    def __compute_metrics_combined(df, val_spigot_prediction, val_overflow_prediction, ibuc):\n",
    "        # Use specific columns for each bucket\n",
    "        spigot_out = df.loc[val_start:val_end, 'q_spigot_b' + str(ibuc)]\n",
    "        spigot_mean = np.mean(spigot_out)\n",
    "        spigot_pred_variance = 0\n",
    "        spigot_obs_variance = 0\n",
    "        spigot_abs_diff = 0\n",
    "\n",
    "        overflow_out = df.loc[val_start:val_end, 'q_overflow_b' + str(ibuc)]\n",
    "        overflow_mean = np.mean(overflow_out)\n",
    "        overflow_pred_variance = 0\n",
    "        overflow_obs_variance = 0\n",
    "        overflow_abs_diff = 0\n",
    "\n",
    "        for i, pred_spigot in enumerate(val_spigot_prediction):\n",
    "            t = i + seq_length - 1\n",
    "            spigot_pred_variance += np.power((pred_spigot - spigot_out.iloc[t]), 2)\n",
    "            spigot_obs_variance += np.power((spigot_mean - spigot_out.iloc[t]), 2)\n",
    "            spigot_abs_diff += np.abs(pred_spigot - spigot_out.iloc[t])\n",
    "\n",
    "        for i, pred_overflow in enumerate(val_overflow_prediction):\n",
    "            t = i + seq_length - 1\n",
    "            overflow_pred_variance += np.power((pred_overflow - overflow_out.iloc[t]), 2)\n",
    "            overflow_obs_variance += np.power((overflow_mean - overflow_out.iloc[t]), 2)            \n",
    "            overflow_abs_diff += np.abs(pred_overflow - overflow_out.iloc[t])\n",
    "\n",
    "        spigot_nse = 1 - (spigot_pred_variance / spigot_obs_variance)\n",
    "        overflow_nse = 1 - (overflow_pred_variance / overflow_obs_variance)\n",
    "        \n",
    "        spigot_rmse = np.sqrt(spigot_pred_variance / len(val_spigot_prediction))\n",
    "        overflow_rmse = np.sqrt(overflow_pred_variance / len(val_overflow_prediction))\n",
    "        \n",
    "        spigot_mae = spigot_abs_diff / len(val_spigot_prediction)\n",
    "        overflow_mae = overflow_abs_diff / len(val_overflow_prediction)\n",
    "\n",
    "        return spigot_nse, overflow_nse, spigot_rmse, overflow_rmse, spigot_mae, overflow_mae\n",
    "\n",
    "    def __compute_mass_balance_combined(df, ibuc):\n",
    "        suffix = '_b' + str(ibuc)  # Suffix to identify the specific bucket\n",
    "        mass_in = df.sum()['precip' + suffix]\n",
    "        mass_out = df.sum()['et' + suffix] + df.sum()['q_overflow' + suffix] + df.sum()['q_spigot' + suffix] + df['h_bucket' + suffix].iloc[-1]\n",
    "        return mass_in, mass_out\n",
    "\n",
    "    if verbose: print(\"*** Network\", inetwork, \"***\")\n",
    "\n",
    "    df = retrieve_combined_data(inetwork, combined_network_dictionary)  # Retrieves the combined DataFrame for this network\n",
    "    metrics_list = []\n",
    "\n",
    "    for ibuc in range(n_buckets_per_network):\n",
    "        # Prediction is made with the combined DataFrame, but specifying the bucket\n",
    "        val_spigot_prediction, val_overflow_prediction = __make_prediction(df, ibuc)\n",
    "        # Compute the metrics for the predictions of this bucket\n",
    "        spigot_nse, overflow_nse, spigot_rmse, overflow_rmse, spigot_mae, overflow_mae = __compute_metrics_combined(df, val_spigot_prediction, val_overflow_prediction, ibuc)\n",
    "        mass_in, mass_out = __compute_mass_balance_combined(df, ibuc)\n",
    "\n",
    "        new_row = {\n",
    "            'spigot_nse': spigot_nse, 'overflow_nse': overflow_nse, \n",
    "            'spigot_rmse': spigot_rmse, 'overflow_rmse': overflow_rmse, \n",
    "            'spigot_mae': spigot_mae, 'overflow_mae': overflow_mae,\n",
    "            'mass_in': mass_in, 'mass_out': mass_out\n",
    "        }\n",
    "        metrics_list.append(new_row)\n",
    "\n",
    "        if verbose:\n",
    "\n",
    "            print(\"Bucket\", ibuc)\n",
    "            print(\"NSE Spigot: \", spigot_nse)\n",
    "            print(\"NSE Overflow: \", overflow_nse)\n",
    "            print(\"RMSE Spigot: \", spigot_rmse)\n",
    "            print(\"RMSE Overflow: \", overflow_rmse)\n",
    "            print(\"MAE Spigot: \", spigot_mae)\n",
    "            print(\"MAE Overflow: \", overflow_mae)\n",
    "            print(\"Mass into the system: \", mass_in)\n",
    "            print(\"Mass out or left over:\", mass_out)\n",
    "            print(\"Percent mass residual: {:.0%}\".format((mass_in - mass_out) / mass_in))\n",
    "\n",
    "            # Plot the predictions for this bucket\n",
    "            fig = plt.figure(figsize=(12, 3))\n",
    "            ax1 = fig.add_subplot(1, 2, 1)\n",
    "            ax2 = fig.add_subplot(1, 2, 2)\n",
    "            ax1.plot(df.loc[val_start + seq_length - 1:val_start + n_plot + seq_length - 1, 'q_spigot_b' + str(ibuc)].values, label = \"Spigot out\")\n",
    "            ax1.plot(val_spigot_prediction[:n_plot], label=\"LSTM spigot out\")\n",
    "            ax1.legend(loc=\"upper right\")\n",
    "            ax1.set_xlabel('Time (hours)')\n",
    "            ax1.set_ylabel('Flow rate (m$^3$/s)')\n",
    "            ax2.plot(df.loc[val_start + seq_length - 1:val_start + n_plot + seq_length - 1, 'q_overflow_b' + str(ibuc)].values, label = \"Overflow\")\n",
    "            ax2.plot(val_overflow_prediction[:n_plot], label=\"LSTM Overflow\")\n",
    "            ax2.legend(loc=\"upper right\")\n",
    "            ax2.set_xlabel('Time (hours)')\n",
    "            ax2.set_ylabel('Flow rate (m$^3$/s)')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    avg_metrics = metrics_df[['spigot_nse', 'overflow_nse', 'spigot_rmse', 'overflow_rmse', 'spigot_mae', 'overflow_mae']].mean()\n",
    "    total_mass_in = metrics_df['mass_in'].sum()\n",
    "    total_mass_out = metrics_df['mass_out'].sum()\n",
    "    total_mass_residual_percent = (total_mass_in - total_mass_out) / total_mass_in\n",
    "\n",
    "    #network_metrics_df = avg_metrics.to_frame().transpose()\n",
    "    network_metrics_df = pd.DataFrame([avg_metrics])\n",
    "    network_metrics_df['total_mass_in'] = total_mass_in\n",
    "    network_metrics_df['total_mass_out'] = total_mass_out\n",
    "    network_metrics_df['total_mass_residual_percent'] = total_mass_residual_percent\n",
    "\n",
    "    if verbose:\n",
    "        print(\"*** Network Metrics ***\")\n",
    "        print(\"Network\", inetwork)\n",
    "        print(\"NSE Spigot: \", network_metrics_df['spigot_nse'].iloc[0])\n",
    "        print(\"NSE Overflow: \", network_metrics_df['overflow_nse'].iloc[0])\n",
    "        print(\"RMSE Spigot: \", network_metrics_df['spigot_rmse'].iloc[0])\n",
    "        print(\"RMSE Overflow: \", network_metrics_df['overflow_rmse'].iloc[0])\n",
    "        print(\"MAE Spigot: \", network_metrics_df['spigot_mae'].iloc[0])\n",
    "        print(\"MAE Overflow: \", network_metrics_df['overflow_mae'].iloc[0])\n",
    "        print(\"Mass into the system: \", network_metrics_df['total_mass_in'].iloc[0])\n",
    "        print(\"Mass out or left over: \", network_metrics_df['total_mass_out'].iloc[0])\n",
    "        print(\"Percent mass residual: {:.0%}\".format(network_metrics_df['total_mass_residual_percent'].iloc[0]))\n",
    "\n",
    "    return network_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating the neural network model (LSTM)\n",
    "\n",
    "Using the hyperparameters from Section 1.2, we define a specific instance of the LSTM model and set up the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "torch.manual_seed(1)\n",
    "lstm_combined = LSTM1(num_classes=n_combined_output,  \n",
    "                        input_size=n_combined_input,    \n",
    "                        hidden_size=hidden_state_size_combined, \n",
    "                        num_layers=num_layers_combined, \n",
    "                        batch_size=batch_size, \n",
    "                        seq_length=seq_length).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_combined_input, n_combined_output, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Setting up the data to feed into the model\n",
    "\n",
    "We will set up data for:\n",
    "- training, to calculate the loss which is backpropogated through the model\n",
    "- validation, where we get predictions from the trained model and see if the performance is up to our standards\n",
    "- testing, the data we will utimately use to report the LSTM performance. \n",
    "\n",
    "Note: testing is the last thing we would do, if we go back to validation after this step, we would be P-hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting a scaler to the training set to transform all the data**\n",
    "\n",
    "Here we fit a scaler to the training set, allowing for the transformation of input and output variables to a normalized and standardized scale, which helps in training the model and maintaining consistency across different datasets. This normalization step ensures that the data is suitable for training our LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_combined_scaler(combined_network_dictionary):\n",
    "    # Adjust scaler for combined input\n",
    "    frames_in = [combined_network_dictionary['networks'][inetwork].loc[train_start:train_end, input_combined_vars] for inetwork in basin_networks_train]\n",
    "    df_in_combined = pd.concat(frames_in)\n",
    "    scaler_in_combined = StandardScaler()\n",
    "    scaler_train_in_combined = scaler_in_combined.fit_transform(df_in_combined)\n",
    "    # Adjust scaler for combined output\n",
    "    frames_out = [combined_network_dictionary['networks'][inetwork].loc[train_start:train_end, output_combined_vars] for inetwork in basin_networks_train]\n",
    "    df_out_combined = pd.concat(frames_out)\n",
    "    scaler_out_combined = StandardScaler()\n",
    "    scaler_train_out_combined = scaler_out_combined.fit_transform(df_out_combined)\n",
    "\n",
    "    return scaler_in_combined, scaler_out_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "scaler_in_combined, scaler_out_combined = fit_combined_scaler(combined_network_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to create data loader for each data split**\n",
    "\n",
    "We prepare and organize the data for training. We create data loaders that handle batch processing and shuffling of the data. We also add some preprocessing steps such as scaling the input and output variables. The data loaders and numpy arrays are used for feeding the data into the neural network during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combined_data_loader(start, end, basin_networks_list, combined_network_dictionary, input_combined_vars, output_combined_vars, scaler_in_combined, scaler_out_combined, n_combined_input):\n",
    "    loader = {}\n",
    "    np_seq_X = {}\n",
    "    np_seq_y = {}\n",
    "    \n",
    "    # Traverse the networks in the provided list\n",
    "    for inetwork in basin_networks_list:\n",
    "        # Assuming each dataframe in the network covers the same time range\n",
    "        df_combined = combined_network_dictionary['networks'][inetwork]\n",
    "        \n",
    "        # Transform the whole dataframe for inputs\n",
    "        scaler_in_i = scaler_in_combined.transform(df_combined.loc[start:end, input_combined_vars])\n",
    "        np_seq_X_ibuc = np.zeros((scaler_in_i.shape[0] - seq_length, seq_length, n_combined_input))\n",
    "        \n",
    "        for i in range(scaler_in_i.shape[0] - seq_length):\n",
    "            t = i + seq_length\n",
    "            np_seq_X_ibuc[i, :, :] = scaler_in_i[i:t, :]\n",
    "\n",
    "        # Prepare the output data\n",
    "        scaler_out_i = scaler_out_combined.transform(df_combined.loc[start:end, output_combined_vars])\n",
    "        np_seq_y_ibuc = np.zeros((scaler_out_i.shape[0] - seq_length, seq_length, n_combined_output))\n",
    "            \n",
    "        for i in range(scaler_out_i.shape[0] - seq_length):\n",
    "            t = i + seq_length\n",
    "            np_seq_y_ibuc[i, :, :] = scaler_out_i[i:t, :]\n",
    "            \n",
    "        np_seq_y[inetwork] = np_seq_y_ibuc\n",
    "\n",
    "        # Create data loaders\n",
    "        ds = torch.utils.data.TensorDataset(torch.Tensor(np_seq_X_ibuc), torch.Tensor(np_seq_y[inetwork]))\n",
    "        loader[inetwork] = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        np_seq_X[inetwork] = np_seq_X_ibuc\n",
    "    \n",
    "    return loader, np_seq_X, np_seq_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function above and parameters defined in the notebook environment to generate the training, validation, test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "combined_train_loader, combined_np_train_seq_X, combined_np_train_seq_y = make_combined_data_loader(train_start, train_end, basin_networks_train, combined_network_dictionary, input_combined_vars, output_combined_vars, scaler_in_combined, scaler_out_combined, n_combined_input)\n",
    "combined_val_loader, combined_np_val_seq_X, combined_np_val_seq_y = make_combined_data_loader(val_start, val_end, basin_networks_val, combined_network_dictionary, input_combined_vars, output_combined_vars, scaler_in_combined, scaler_out_combined, n_combined_input)\n",
    "combined_test_loader, combined_np_test_seq_X, combined_np_test_seq_y = make_combined_data_loader(test_start, test_end, basin_networks_test, combined_network_dictionary, input_combined_vars, output_combined_vars, scaler_in_combined, scaler_out_combined, n_combined_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the model: Learning the general response of the example dynamic ''hydrologic\" system\n",
    "Now is the time to train the model! Everything above was done in preparation for this step.\n",
    "\n",
    "Here we define a function to train the LSTM neural network model with the nn.MSELoss() loss function and using the Adam optimizer and hyperparameters defined above.\n",
    "\n",
    "**Brief explanation**:\n",
    "- The training is done for a specified number of epochs.\n",
    "- For each epoch, the individual characteristics of each bucket and the combined flow of the entire network are processed.\n",
    "- Data from each basin network are loaded using a PyTorch DataLoader and passed through the LSTM model to predict the individual flow and overflow for each bucket in the network.\n",
    "- The output is then compared with the target values using the custom loss function. \n",
    "- The gradients are calculated and the optimizer is used to update the weights of the model. \n",
    "- We use the tqdm library to show the progress of the training. \n",
    "- Finally, we estimate the average RMSE for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(lstm, train_loader, basin_networks_train, learning_rate, num_epochs):\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = criterion.to(device=device) \n",
    "        \n",
    "    optimizer = optim.Adam(lstm.parameters(), lr = learning_rate[0])\n",
    "    epoch_bar = tqdm(range(num_epochs), desc = \"Training\", position = 0, total = num_epochs)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    for epoch in epoch_bar:\n",
    "        for inetwork in basin_networks_train:\n",
    "            # Initialize results for each network at the beginning\n",
    "            if inetwork not in results:\n",
    "                results[inetwork] = {\"loss\": [], \"RMSE\": []}\n",
    "\n",
    "            batch_bar = tqdm(enumerate(train_loader[inetwork]),\n",
    "                             desc=\"Network: {}, Epoch: {}\".format(str(inetwork), str(epoch)),\n",
    "                             position = 1, total = len(train_loader[inetwork]), leave = False, disable = True)\n",
    "\n",
    "            for i, (data, targets) in batch_bar:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                optimizer = optim.Adam(lstm.parameters(), lr = learning_rate[epoch])\n",
    "\n",
    "                data = data.to(device = device)\n",
    "                targets = targets.to(device = device)\n",
    "\n",
    "                # Forward\n",
    "                lstm_output = lstm(data)\n",
    "                loss = criterion(lstm_output, targets)\n",
    "\n",
    "                #backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_bar.set_postfix(loss = loss.to(device).item(),\n",
    "                                      RMSE = \"{:.2f}\".format(loss ** (1 / 2)),\n",
    "                                      epoch = epoch)\n",
    "                batch_bar.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rmse_list = []\n",
    "                for i, (data_, targets_) in enumerate(train_loader[inetwork]):\n",
    "                    data_ = data_.to(device = device)\n",
    "                    targets_ = targets_.to(device = device)\n",
    "\n",
    "                    lstm_output_ = lstm(data_)\n",
    "                    MSE_ = criterion(lstm_output_, targets_)\n",
    "                    rmse_list.append(MSE_ ** (1 / 2))\n",
    "\n",
    "            #meanrmse = np.mean(np.array(torch.Tensor(rmse_list)))\n",
    "            meanrmse = torch.stack(rmse_list).mean().item()\n",
    "            epoch_bar.set_postfix(loss = loss.cpu().item(),\n",
    "                                  RMSE = \"{:.2f}\".format(meanrmse),\n",
    "                                  epoch = epoch)\n",
    "\n",
    "            results[inetwork][\"loss\"].append(loss.cpu().item())\n",
    "            results[inetwork][\"RMSE\"].append(meanrmse)\n",
    "\n",
    "            batch_bar.update()\n",
    "\n",
    "        #clear_output()\n",
    "\n",
    "    return lstm, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the train model functon by prescribing the buckets and data to use for training and the intantiated LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "lstm_combined, results_combined = train_model(lstm_combined, combined_train_loader, basin_networks_train, learning_rate_combined, num_epochs_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualizing the learning curves\n",
    "\n",
    " We plot the loss and root mean square error (RMSE) metrics for each epoch to check if the model fitting has converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_learning_curve(results):\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    for inetwork in basin_networks_train:\n",
    "        num_epochs_executed = len(results[inetwork]['loss'])\n",
    "        ax1.plot(range(num_epochs_executed), results[inetwork]['loss'])\n",
    "        ax2.plot(range(num_epochs_executed), results[inetwork]['RMSE'])\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    plt.suptitle(\"Learning curves for each network\") \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "viz_learning_curve(results_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Checking that the model works on the validation data\n",
    "Now that we have a trained model, we can see how it works on our validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_metrics(metrics_list, title):\n",
    "    metrics_df = pd.DataFrame(metrics_list).mean()\n",
    "    print(f\"*** Model Metrics - {title} ***\")\n",
    "    print(\"Average NSE Spigot: \", metrics_df['spigot_nse'])\n",
    "    print(\"Average NSE Overflow: \", metrics_df['overflow_nse'])\n",
    "    print(\"Average RMSE Spigot: \", metrics_df['spigot_rmse'])\n",
    "    print(\"Average RMSE Overflow: \", metrics_df['overflow_rmse'])\n",
    "    print(\"Average MAE Spigot: \", metrics_df['spigot_mae'])\n",
    "    print(\"Average MAE Overflow: \", metrics_df['overflow_mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "model_combined_metrics_list = []\n",
    "for inetwork in basin_networks_val:\n",
    "    network_metrics_df = check_validation_period_combined(lstm_combined, combined_np_val_seq_X, combined_network_dictionary, inetwork, n_plot=100, verbose=False)\n",
    "    model_combined_metrics_list.append(network_metrics_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "print_model_metrics(model_combined_metrics_list, \"Validation: Combined bucket network inputs with combined downstream flow data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final model evaluation\n",
    "\n",
    "Finally, we evaluate the model on the test data to see how well it performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test_period_combined(lstm_combined, combined_np_test_seq_X, inetwork, n_plot=100, verbose=False):\n",
    "    def __make_prediction(df, ibuc):\n",
    "        lstm_output_test = lstm_combined(torch.Tensor(combined_np_test_seq_X[inetwork]).to(device=device))\n",
    "        test_spigot_prediction = []\n",
    "        test_overflow_prediction = []\n",
    "\n",
    "        for i in range(lstm_output_test.shape[0]):\n",
    "            spigot_pred = (lstm_output_test[i, -1, (ibuc * 2) + 1].cpu().detach().numpy() * np.std(df.loc[train_start:train_end, 'q_spigot_b' + str(ibuc)])) + np.mean(df.loc[train_start:train_end, 'q_spigot_b' + str(ibuc)])\n",
    "            overflow_pred = (lstm_output_test[i, -1, (ibuc * 2)].cpu().detach().numpy() * np.std(df.loc[train_start:train_end, 'q_overflow_b' + str(ibuc)])) + np.mean(df.loc[train_start:train_end, 'q_overflow_b' + str(ibuc)])\n",
    "\n",
    "            spigot_pred = max(spigot_pred, 0)\n",
    "            overflow_pred = max(overflow_pred, 0)\n",
    "\n",
    "            test_spigot_prediction.append(spigot_pred)\n",
    "            test_overflow_prediction.append(overflow_pred)\n",
    "\n",
    "        return test_spigot_prediction, test_overflow_prediction\n",
    "\n",
    "    def __compute_metrics_combined(df, test_spigot_prediction, test_overflow_prediction, ibuc):\n",
    "        # Use specific columns for each bucket\n",
    "        spigot_out = df.loc[test_start:test_end, 'q_spigot_b' + str(ibuc)]\n",
    "        spigot_mean = np.mean(spigot_out)\n",
    "        spigot_pred_variance = 0\n",
    "        spigot_obs_variance = 0\n",
    "        spigot_abs_diff = 0\n",
    "\n",
    "        overflow_out = df.loc[test_start:test_end, 'q_overflow_b' + str(ibuc)]\n",
    "        overflow_mean = np.mean(overflow_out)\n",
    "        overflow_pred_variance = 0\n",
    "        overflow_obs_variance = 0\n",
    "        overflow_abs_diff = 0\n",
    "\n",
    "        for i, pred_spigot in enumerate(test_spigot_prediction):\n",
    "            t = i + seq_length - 1\n",
    "            spigot_pred_variance += np.power((pred_spigot - spigot_out.iloc[t]), 2)\n",
    "            spigot_obs_variance += np.power((spigot_mean - spigot_out.iloc[t]), 2)\n",
    "            spigot_abs_diff += np.abs(pred_spigot - spigot_out.iloc[t])\n",
    "\n",
    "        for i, pred_overflow in enumerate(test_overflow_prediction):\n",
    "            t = i + seq_length - 1\n",
    "            overflow_pred_variance += np.power((pred_overflow - overflow_out.iloc[t]), 2)\n",
    "            overflow_obs_variance += np.power((overflow_mean - overflow_out.iloc[t]), 2)            \n",
    "            overflow_abs_diff += np.abs(pred_overflow - overflow_out.iloc[t])\n",
    "\n",
    "        spigot_nse = 1 - (spigot_pred_variance / spigot_obs_variance)\n",
    "        overflow_nse = 1 - (overflow_pred_variance / overflow_obs_variance)\n",
    "        \n",
    "        spigot_rmse = np.sqrt(spigot_pred_variance / len(test_spigot_prediction))\n",
    "        overflow_rmse = np.sqrt(overflow_pred_variance / len(test_overflow_prediction))\n",
    "        \n",
    "        spigot_mae = spigot_abs_diff / len(test_spigot_prediction)\n",
    "        overflow_mae = overflow_abs_diff / len(test_overflow_prediction)\n",
    "\n",
    "        return spigot_nse, overflow_nse, spigot_rmse, overflow_rmse, spigot_mae, overflow_mae\n",
    "\n",
    "    def __compute_mass_balance_combined(df, ibuc):\n",
    "        suffix = '_b' + str(ibuc)  # Suffix to identify the specific bucket\n",
    "        mass_in = df.sum()['precip' + suffix]\n",
    "        mass_out = df.sum()['et' + suffix] + df.sum()['q_overflow' + suffix] + df.sum()['q_spigot' + suffix] + df['h_bucket' + suffix].iloc[-1]\n",
    "        return mass_in, mass_out\n",
    "\n",
    "    if verbose: print(\"*** Network\", inetwork, \"***\")\n",
    "\n",
    "    df = retrieve_combined_data(inetwork, combined_network_dictionary)  # Retrieves the combined DataFrame for this network\n",
    "    metrics_list = []\n",
    "\n",
    "    for ibuc in range(n_buckets_per_network):\n",
    "        # Prediction is made with the combined DataFrame, but specifying the bucket\n",
    "        test_spigot_prediction, test_overflow_prediction = __make_prediction(df, ibuc)\n",
    "        # Compute the metrics for the predictions of this bucket\n",
    "        spigot_nse, overflow_nse, spigot_rmse, overflow_rmse, spigot_mae, overflow_mae = __compute_metrics_combined(df, test_spigot_prediction, test_overflow_prediction, ibuc)\n",
    "        mass_in, mass_out = __compute_mass_balance_combined(df, ibuc)\n",
    "\n",
    "        new_row = {\n",
    "            'spigot_nse': spigot_nse, 'overflow_nse': overflow_nse, \n",
    "            'spigot_rmse': spigot_rmse, 'overflow_rmse': overflow_rmse, \n",
    "            'spigot_mae': spigot_mae, 'overflow_mae': overflow_mae,\n",
    "            'mass_in': mass_in, 'mass_out': mass_out\n",
    "        }\n",
    "        metrics_list.append(new_row)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Bucket\", ibuc)\n",
    "            print(\"NSE Spigot: \", spigot_nse)\n",
    "            print(\"NSE Overflow: \", overflow_nse)\n",
    "            print(\"RMSE Spigot: \", spigot_rmse)\n",
    "            print(\"RMSE Overflow: \", overflow_rmse)\n",
    "            print(\"MAE Spigot: \", spigot_mae)\n",
    "            print(\"MAE Overflow: \", overflow_mae)\n",
    "            print(\"Mass into the system: \", mass_in)\n",
    "            print(\"Mass out or left over:\", mass_out)\n",
    "            print(\"Percent mass residual: {:.0%}\".format((mass_in - mass_out) / mass_in))\n",
    "\n",
    "            # Plot the predictions for this bucket\n",
    "            fig = plt.figure(figsize=(12, 3))\n",
    "            ax1 = fig.add_subplot(1, 2, 1)\n",
    "            ax2 = fig.add_subplot(1, 2, 2)\n",
    "            ax1.plot(df.loc[test_start + seq_length - 1:test_start + n_plot + seq_length - 1, 'q_spigot_b' + str(ibuc)].values, label = \"Spigot out\")\n",
    "            ax1.plot(test_spigot_prediction[:n_plot], label=\"LSTM spigot out\")\n",
    "            ax1.legend(loc=\"upper right\")\n",
    "            ax1.set_xlabel('Time (hours)')\n",
    "            ax1.set_ylabel('Flow rate (m$^3$/s)')\n",
    "            ax2.plot(df.loc[test_start + seq_length - 1:test_start + n_plot + seq_length - 1, 'q_overflow_b' + str(ibuc)].values, label = \"Overflow\")\n",
    "            ax2.plot(test_overflow_prediction[:n_plot], label=\"LSTM Overflow\")\n",
    "            ax2.legend(loc=\"upper right\")\n",
    "            ax2.set_xlabel('Time (hours)')\n",
    "            ax2.set_ylabel('Flow rate (m$^3$/s)')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    avg_metrics = metrics_df[['spigot_nse', 'overflow_nse', 'spigot_rmse', 'overflow_rmse', 'spigot_mae', 'overflow_mae']].mean()\n",
    "    total_mass_in = metrics_df['mass_in'].sum()\n",
    "    total_mass_out = metrics_df['mass_out'].sum()\n",
    "    total_mass_residual_percent = (total_mass_in - total_mass_out) / total_mass_in\n",
    "\n",
    "    network_metrics_df = pd.DataFrame([avg_metrics])\n",
    "    network_metrics_df['total_mass_in'] = total_mass_in\n",
    "    network_metrics_df['total_mass_out'] = total_mass_out\n",
    "    network_metrics_df['total_mass_residual_percent'] = total_mass_residual_percent\n",
    "\n",
    "    if verbose:\n",
    "        print(\"*** Network Metrics ***\")\n",
    "        print(\"Network\", inetwork)\n",
    "        print(\"NSE Spigot: \", network_metrics_df['spigot_nse'].iloc[0])\n",
    "        print(\"NSE Overflow: \", network_metrics_df['overflow_nse'].iloc[0])\n",
    "        print(\"RMSE Spigot: \", network_metrics_df['spigot_rmse'].iloc[0])\n",
    "        print(\"RMSE Overflow: \", network_metrics_df['overflow_rmse'].iloc[0])\n",
    "        print(\"MAE Spigot: \", network_metrics_df['spigot_mae'].iloc[0])\n",
    "        print(\"MAE Overflow: \", network_metrics_df['overflow_mae'].iloc[0])\n",
    "        print(\"Mass into the system: \", network_metrics_df['total_mass_in'].iloc[0])\n",
    "        print(\"Mass out or left over: \", network_metrics_df['total_mass_out'].iloc[0])\n",
    "        print(\"Percent mass residual: {:.0%}\".format(network_metrics_df['total_mass_residual_percent'].iloc[0]))\n",
    "\n",
    "    return network_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "test_model_combined_metrics_list = []\n",
    "for inetwork in basin_networks_test:\n",
    "    network_metrics_df = check_test_period_combined(lstm_combined, combined_np_test_seq_X, inetwork, n_plot=100, verbose=False)\n",
    "    test_model_combined_metrics_list.append(network_metrics_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combined bucket network inputs with combined downstream flow data\n",
    "print_model_metrics(test_model_combined_metrics_list, \"Testing: Combined bucket network inputs with combined downstream flow data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
